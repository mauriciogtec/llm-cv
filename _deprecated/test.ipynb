{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mag7273/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import nltk\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import ArxivLoader, PyPDFLoader\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_papers = {\n",
    "    \"AIM\": \"2105.13345\",\n",
    "}\n",
    "\n",
    "papers = ArxivLoader(arxiv_papers[\"AIM\"]).load()\n",
    "cv = PyPDFLoader(\"https://mauriciogtec.com/_static/cv.pdf\").load()\n",
    "docs = papers + cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 739, which is longer than the specified 500\n",
      "Created a chunk of size 730, which is longer than the specified 500\n",
      "Created a chunk of size 558, which is longer than the specified 500\n",
      "Created a chunk of size 644, which is longer than the specified 500\n",
      "Created a chunk of size 1154, which is longer than the specified 500\n",
      "Created a chunk of size 819, which is longer than the specified 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Adversarial Intrinsic Motivation for Reinforcement\\nLearning\\nIshan Durugkar\\nDepartment of Computer Science\\nThe University of Texas at Austin\\nAustin, TX, USA 78703\\nishand@cs.utexas.edu\\nMauricio Tec\\nDepartment of Statistics and Data Sciences\\nThe University of Texas at Austin\\nAustin, TX, USA 78703\\nmauriciogtec@utexas.edu\\nScott Niekum\\nDepartment of Computer Science\\nThe University of Texas at Austin\\nAustin, TX, USA 78703\\nsniekum@cs.utexas.edu\\nPeter Stone\\nDepartment of Computer Science\\nThe University of Texas at Austin\\nAustin, TX, USA 78703 and\\nSony AI\\npstone@cs.utexas.edu\\nAbstract\\nLearning with an objective to minimize the mismatch with a reference distribution\\nhas been shown to be useful for generative modeling and imitation learning.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='In\\nthis paper, we investigate whether one such objective, the Wasserstein-1 distance\\nbetween a policy’s state visitation distribution and a target distribution, can be\\nutilized effectively for reinforcement learning (RL) tasks.\\n\\nSpeciﬁcally, this paper\\nfocuses on goal-conditioned reinforcement learning where the idealized (unachiev-\\nable) target distribution has full measure at the goal.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='This paper introduces a\\nquasimetric speciﬁc to Markov Decision Processes (MDPs) and uses this quasimet-\\nric to estimate the above Wasserstein-1 distance.\\n\\nIt further shows that the policy\\nthat minimizes this Wasserstein-1 distance is the policy that reaches the goal in\\nas few steps as possible.\\n\\nOur approach, termed Adversarial Intrinsic Motivation\\n(AIM), estimates this Wasserstein-1 distance through its dual objective and uses it\\nto compute a supplemental reward function.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs the\\nagent’s exploration to ﬁnd the goal efﬁciently.\\n\\nAdditionally, we combine AIM with\\nHindsight Experience Replay (HER) and show that the resulting algorithm acceler-\\nates learning signiﬁcantly on several simulated robotics tasks when compared to\\nother rewards that encourage exploration or accelerate learning.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='1\\nIntroduction\\nReinforcement Learning (RL) [70] deals with the problem of learning a policy to accomplish a given\\ntask in an optimal manner.\\n\\nThis task is typically communicated to the agent by means of a reward\\nfunction.\\n\\nIf the reward function is sparse [4] (e.g., most transitions yield a reward of 0), much random\\nexploration might be needed before the agent experiences any signal relevant to learning [11, 2].', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='Some of the different ways to speed up reinforcement learning by modifying or augmenting the reward\\nfunction are shaped rewards [51], redistributed rewards [2], intrinsic motivations [8, 65, 67, 68, 53, 56],\\nand learned rewards [77, 53].\\n\\nUnfortunately, the optimal policy under such modiﬁed rewards might\\nsometimes be different than the optimal policy under the task reward [51, 18].\\n\\nThe problem of\\nPreprint.\\n\\nUnder review.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='The problem of\\nPreprint.\\n\\nUnder review.\\n\\narXiv:2105.13345v3  [cs.LG]  28 Oct 2021\\nlearning a reward signal that speeds up learning by communicating what to do but does not interfere\\nby specifying how to do it is thus a useful and complex one [78].', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='This work considers whether a task-dependent reward function learned based on the distribution\\nmismatch between the agent’s state visitation distribution and a target task (expressed as a distribution)\\ncan guide the agent towards accomplishing this task.\\n\\nAdversarial methods to minimize distribution\\nmismatch have been used with great success in generative modeling [28] and imitation learning\\n[38, 24, 75, 72, 27].', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='In both these scenarios, the task is generally to minimize the mismatch with a\\ntarget distribution induced by the data or expert demonstrations.\\n\\nInstead, we consider the task of\\ngoal-conditioned RL, where the ideal target distribution assigns full measure to a goal state.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='While\\nthe agent can never match this idealized target distribution perfectly unless starting at the goal,\\nintuitively, minimizing the mismatch with this distribution should lead to trajectories that maximize\\nthe proportion of the time spent at the goal, thereby prioritizing transitions essential to doing so.\\n\\nThe theory of optimal transport [74] gives us a way to measure the distance between two distributions\\n(called the Wasserstein distance) even if they have disjoint support.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "splitter = NLTKTextSplitter(chunk_overlap=100, chunk_size=500)\n",
    "splits = splitter.split_documents(docs)\n",
    "splits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2021-10-28',\n",
       " 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning',\n",
       " 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone',\n",
       " 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mag7273/mambaforge/envs/llm-cv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAIEmbeddings instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.embed_documents(\n",
    "    \"Adversarial Intrinsic Motivation for Reinforcement\\nLearning\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf ./docs/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./docs/chroma\"\n",
    "vectordb = Chroma.from_documents(\n",
    "    splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize Mauricio's skills\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Mauricio Tec\\nGoogle Scholar ὑ7mauriciogtec.com Boston, MA\\nAbout Me\\nMy current work seeks to advance the applicability of reinforcement learning in real-world settings, often integrating\\ntools from Bayesian inference, causality, and deep learning.\\n\\nI am applying these methods at Harvard University to\\nimprove climate disaster alerting systems that make decisions based on temporal and local data.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='B.Sc.\\n\\nin Applied Mathematics, Instituto Tecnologico Autonomo de Mexico (ITAM), 2007–2012\\nWork Experience\\nHarvard University, Postdoctoral Research Fellow/Research Associate , 2022–date\\n•Designing critical deep reinforcement learning applications to optimize the issuance of US heat alerts that utilize\\ndaily timeseries and forecasts to make smart decisions about when and how to take action.\\n\\n•Writing and publish-\\ning papers and software in top ML conferences and stats journals.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='ITAM,Lecturer, Applied Mathematics Department , 2015–2017\\nTaught courses in computational statistics and stochastic processes to undergraduate students.\\n\\nCIDAC,Data Analyst , 2013–2014\\nConducted data analysis supporting the Mexican think tank CIDAC’s economic policy recommendation.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='CIBanco Financial Asset Management, Data Scientist , 2016–2017\\n•Applied statistical and ML methods (risk parities, bayesian shrinkage, forecasting) to optimize multi-asset class\\ninvestments.\\n\\n•Integrated production-level analytics with the bank’s financial information streamed databases.\\n\\n•Actively participated in weekly committees with senior stakeholders, influencing investment decisions.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='•Writing and publish-\\ning papers and software in top ML conferences and stats journals.\\n\\n•Key role in writing four grant proposals (two\\nfunded by NSF/NIH).\\n\\n•Mentoring doctoral students.\\n\\n•Promoted to research associate in 09/23.\\n\\nFacebook AI Research (FAIR), Research Intern , 2020\\nImplemented world-model reinforcement learning models, performing code optimizations and algorithmic improve-\\nments to outperform the original Dreamer implementation for MuJoCo robotic simulator benchmarks.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='Skills\\n•Programming Languages : Python (preferred); Julia, R (proficient); C++ (intermediate);\\n•High-performance Computing : Slurm (advanced); AWS/Azure Cloud (intermediate);\\n•Data Science : SQL (advanced); tidyverse, ggplot, pandas, ggplot, networkx (proficient); NLP (advanced);\\n•Development and Pipelines : Git, Docker, SnakeFlow, Linux (advanced);\\n•Deep Learning : PyTorch, Tensorflow, Image segmentation/Object detection, Spatial methods, (proficient); GNNs,\\nAttention-based models (advanced); Multi-GPU, Distributed training (intermediate);\\n•Experiment Design/ Sequential Decision-making : Active learning, Reinforcement learning, Tree search (proficient);\\n•Statistics: Bayesian inference, Causal inference, A/B testing (proficient);\\nScholarships, Grants, and Awards\\n-NIH Supplement 3RF1AG080948-01S1 (2023–2025).', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 1}),\n",
       " Document(page_content='My ultimate goal\\nis to develop trustworthy AI systems that make decisions for and with humans in real-world applications.\\n\\nEducation\\nPh.D. in Statistics, The University of Texas at Austin, 2017–2022\\n•Developed Bayesian and causal deep learning methods supervised by James Scott and Corwin Zigler at the\\nstatistics department.\\n\\n•Conducted reinforcement learning and robotics research in theLearning Agents Research\\nGroup(LARG) directed by Peter Stone at the computer science department.\\n\\nM.Sc.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='Developed a miti-\\ngation strategy to increase resiliency.\\n\\nTested on ResNet ImageNet benchmarks.\\n\\nThe University of Texas at Austin, Graduate Research Assistant , 2019–2021\\nDeveloped a new self-supervised method for causal inference with spatial dependencies, evaluating it in down-\\nstream air pollution studies, allowing the use of simpler causal inference methods for otherwise complex tasks.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='This work has taken place in part in the Learning Agents Research Group (LARG) at the Artiﬁcial\\nIntelligence Laboratory, and in part in the Personal Autonomous Robotics Lab (PeARL) at The\\nUniversity of Texas at Austin.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='Williamson S and Tec M. “Random clique covers for graphs with local density and global sparsity”.\\n\\nIn: Uncer-\\ntainty in Artificial Intelligence (UAI) (2019)\\nThe full list of publications and work under review is available on my Google Scholar.\\n\\nI have over 24 publications\\nand 500 citations, including collaborative work in various domains such as robotics, climate, health, transportation,\\nand computational biology.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = vectordb.search(query, search_type=\"similarity\", k=10)\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Published\",\n",
    "        type=\"date\",\n",
    "        description=\"Date of paper/publication\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Title\",\n",
    "        type=\"string\",\n",
    "        description=\"Title of the paper/publication\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Summary\",\n",
    "        type=\"string\",\n",
    "        description=\"Summary of the paper/publication\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Authors\",\n",
    "        type=\"string\",\n",
    "        description=\"Authors of the paper/publication\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        type=\"string\",\n",
    "        description=\"URL or source of the file in case of a pdf\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        type=\"integer\",\n",
    "        description=\"Page number in case of a pdf\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"CV information and research publications\"\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Skills\\n•Programming Languages : Python (preferred); Julia, R (proficient); C++ (intermediate);\\n•High-performance Computing : Slurm (advanced); AWS/Azure Cloud (intermediate);\\n•Data Science : SQL (advanced); tidyverse, ggplot, pandas, ggplot, networkx (proficient); NLP (advanced);\\n•Development and Pipelines : Git, Docker, SnakeFlow, Linux (advanced);\\n•Deep Learning : PyTorch, Tensorflow, Image segmentation/Object detection, Spatial methods, (proficient); GNNs,\\nAttention-based models (advanced); Multi-GPU, Distributed training (intermediate);\\n•Experiment Design/ Sequential Decision-making : Active learning, Reinforcement learning, Tree search (proficient);\\n•Statistics: Bayesian inference, Causal inference, A/B testing (proficient);\\nScholarships, Grants, and Awards\\n-NIH Supplement 3RF1AG080948-01S1 (2023–2025).', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 1}),\n",
       " Document(page_content='B.Sc.\\n\\nin Applied Mathematics, Instituto Tecnologico Autonomo de Mexico (ITAM), 2007–2012\\nWork Experience\\nHarvard University, Postdoctoral Research Fellow/Research Associate , 2022–date\\n•Designing critical deep reinforcement learning applications to optimize the issuance of US heat alerts that utilize\\ndaily timeseries and forecasts to make smart decisions about when and how to take action.\\n\\n•Writing and publish-\\ning papers and software in top ML conferences and stats journals.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0}),\n",
       " Document(page_content='[10] Andrew G Barto, Satinder Singh, and Nuttapong Chentanez.\\n\\nIntrinsically motivated learning\\nof hierarchical collections of skills.\\n\\nIn Proceedings of the 3rd International Conference on\\nDevelopment and Learning, pages 112–19.\\n\\nCambridge, MA, 2004.\\n\\n11\\n[11] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi\\nMunos.\\n\\nUnifying count-based exploration and intrinsic motivation.\\n\\nIn Advances in neural\\ninformation processing systems, pages 1471–1479, 2016.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"}),\n",
       " Document(page_content='This work has taken place in part in the Learning Agents Research Group (LARG) at the Artiﬁcial\\nIntelligence Laboratory, and in part in the Personal Autonomous Robotics Lab (PeARL) at The\\nUniversity of Texas at Austin.', metadata={'Published': '2021-10-28', 'Title': 'Adversarial Intrinsic Motivation for Reinforcement Learning', 'Authors': 'Ishan Durugkar, Mauricio Tec, Scott Niekum, Peter Stone', 'Summary': \"Learning with an objective to minimize the mismatch with a reference\\ndistribution has been shown to be useful for generative modeling and imitation\\nlearning. In this paper, we investigate whether one such objective, the\\nWasserstein-1 distance between a policy's state visitation distribution and a\\ntarget distribution, can be utilized effectively for reinforcement learning\\n(RL) tasks. Specifically, this paper focuses on goal-conditioned reinforcement\\nlearning where the idealized (unachievable) target distribution has full\\nmeasure at the goal. This paper introduces a quasimetric specific to Markov\\nDecision Processes (MDPs) and uses this quasimetric to estimate the above\\nWasserstein-1 distance. It further shows that the policy that minimizes this\\nWasserstein-1 distance is the policy that reaches the goal in as few steps as\\npossible. Our approach, termed Adversarial Intrinsic Motivation (AIM),\\nestimates this Wasserstein-1 distance through its dual objective and uses it to\\ncompute a supplemental reward function. Our experiments show that this reward\\nfunction changes smoothly with respect to transitions in the MDP and directs\\nthe agent's exploration to find the goal efficiently. Additionally, we combine\\nAIM with Hindsight Experience Replay (HER) and show that the resulting\\nalgorithm accelerates learning significantly on several simulated robotics\\ntasks when compared to other rewards that encourage exploration or accelerate\\nlearning.\"})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = retriever.get_relevant_documents(\"Skills from CV\")\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Skills\\n•Programming Languages : Python (preferred); Julia, R (proficient); C++ (intermediate);\\n•High-performance Computing : Slurm (advanced); AWS/Azure Cloud (intermediate);\\n•Data Science : SQL (advanced); tidyverse, ggplot, pandas, ggplot, networkx (proficient); NLP (advanced);\\n•Development and Pipelines : Git, Docker, SnakeFlow, Linux (advanced);\\n•Deep Learning : PyTorch, Tensorflow, Image segmentation/Object detection, Spatial methods, (proficient); GNNs,\\nAttention-based models (advanced); Multi-GPU, Distributed training (intermediate);\\n•Experiment Design/ Sequential Decision-making : Active learning, Reinforcement learning, Tree search (proficient);\\n•Statistics: Bayesian inference, Causal inference, A/B testing (proficient);\\nScholarships, Grants, and Awards\\n-NIH Supplement 3RF1AG080948-01S1 (2023–2025).', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 1}),\n",
       " Document(page_content='ITAM,Lecturer, Applied Mathematics Department , 2015–2017\\nTaught courses in computational statistics and stochastic processes to undergraduate students.\\n\\nCIDAC,Data Analyst , 2013–2014\\nConducted data analysis supporting the Mexican think tank CIDAC’s economic policy recommendation.', metadata={'source': 'https://mauriciogtec.com/_static/cv.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.max_marginal_relevance_search(\"Skills from CV\", k=2, fetch_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mag7273/mambaforge/envs/llm-cv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/mag7273/mambaforge/envs/llm-cv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/mag7273/mambaforge/envs/llm-cv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "/Users/mag7273/mambaforge/envs/llm-cv/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "•Programming Languages : Python (preferred); Julia, R (proficient); C++ (intermediate);\n",
      "•High-performance Computing : Slurm (advanced); AWS/Azure Cloud (intermediate);\n",
      "•Data Science : SQL (advanced); tidyverse, ggplot, pandas, ggplot, networkx (proficient); NLP (advanced);\n",
      "•Development and Pipelines : Git, Docker, SnakeFlow, Linux (advanced);\n",
      "•Deep Learning : PyTorch, Tensorflow, Image segmentation/Object detection, Spatial methods, (proficient); GNNs,\n",
      "Attention-based models (advanced); Multi-GPU, Distributed training (intermediate);\n",
      "•Experiment Design/ Sequential Decision-making : Active learning, Reinforcement learning, Tree search (proficient);\n",
      "•Statistics: Bayesian inference, Causal inference, A/B testing (proficient);\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "ITAM,Lecturer, Applied Mathematics Department , 2015–2017\n",
      "Taught courses in computational statistics and stochastic processes to undergraduate students.\n",
      "\n",
      "CIDAC,Data Analyst , 2013–2014\n",
      "Conducted data analysis supporting the Mexican think tank CIDAC’s economic policy recommendation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "- Published a lightweight real-time robotic computer vision framework for detecting soccer field objects; trained the deep learning model in PyTorch for super low-resolution YUYV robot vision and transferred it to TFLite.\n",
      "- Competed for the UT Austin Villa team using our proposed model implemented in our in-house robotic OS and C++, achieving fourth and fifth place in the SPL league (2020, 2021).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Slide is similar to Push, except the coefﬁcient of friction on the table is reduced (causing pushed objects to slide) and the potential targets are over a larger area, meaning that the robot needs to learn to hit objects towards the goal with the right amount of force.\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "question = \"Skills from CV\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    1\n",
    "    if (1 > 0) else\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
